#  Evoastra Image Captioning System

### **AI-powered Image Caption Generator using BLIP + FastAPI + HTML/JS UI**

The Evoastra Image Captioning System is a complete end-to-end application that generates **human-like captions** for images using a state-of-the-art **Vision‚ÄìLanguage Transformer (BLIP)**.
It also computes a **BLEU-based confidence score** to measure caption stability.


#  Features

* ‚úî **Pretrained BLIP model** for high-quality captioning
* ‚úî **FastAPI backend** for fast inference
* ‚úî **Modern frontend UI** with preview, spinner, and caption display
* ‚úî **BLEU confidence score** to evaluate caption reliability
* ‚úî Supports CPU (no GPU required)
* ‚úî No training required
* ‚úî Lightweight and easy to deploy

---

#  Model Used

### **1Ô∏è‚É£ BLIP (Bootstrapping Language-Image Pretraining)**

* Model: `Salesforce/blip-image-captioning-base`
* Type: Vision‚ÄìLanguage Transformer
* Encoder: Vision Transformer (ViT)
* Decoder: GPT-like text generator
* Reason: High accuracy, zero training needed, works well on CPU

### **2Ô∏è‚É£ Prior Attempted Model (Not used in final)**

* CNN‚ÄìLSTM architecture
* Encoder: ResNet-50
* Decoder: LSTM
* Replaced due to slow training and lower accuracy

---

#  Evaluation Metric Used

###  **BLEU Score (Bilingual Evaluation Understudy)**

* Standard metric in image captioning
* Measures similarity between two sentences
* BLEU = 0 ‚Üí completely different
* BLEU = 1 ‚Üí identical

### Why BLEU Works Here

Since uploaded images have **no ground truth caption**, we use:
‚úî **Caption A** ‚Äì greedy decoding
‚úî **Caption B** ‚Äì sampling-based decoding

Then compute BLEU(A, B) to measure **caption stability**.

Interpretation:

| BLEU Range | Confidence |
| ---------- | ---------- |
| 0.8 ‚Äì 1.0  | High       |
| 0.5 ‚Äì 0.8  | Moderate   |
| < 0.5      | Low        |

---

#  Project Folder Structure

```
MAJOR PROJECT/
‚îÇ‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ caption_api.py          # FastAPI backend with BLIP + BLEU
‚îÇ   ‚îú‚îÄ‚îÄ other backend files
‚îÇ
‚îú‚îÄ‚îÄ index.html                  # Complete frontend UI
‚îú‚îÄ‚îÄ .gitignore                  # Ignores dataset, venv, cache, model files
‚îú‚îÄ‚îÄ README.md                   # Project documentation
‚îî‚îÄ‚îÄ venv/                       # Virtual environment (ignored)
```

---

#  Installation

## 1Ô∏è‚É£ Clone the repository

```bash
git clone https://github.com/Abin-Binu123/Evoastra_Major_Project.git
cd Evoastra_Major_Project
```

## 2Ô∏è‚É£ Create a virtual environment

```bash
python -m venv venv
```

## 3Ô∏è‚É£ Activate venv

Windows:

```bash
venv\Scripts\activate
```

## 4Ô∏è‚É£ Install dependencies

```bash
pip install fastapi uvicorn pillow torch torchvision transformers nltk python-multipart
```

## 5Ô∏è‚É£ Download NLTK tokenizer

```python
import nltk
nltk.download('punkt')
```

---

# Running the Backend

```bash
uvicorn backend.caption_api:app --port 8080
```

You should see:

```
Uvicorn running on http://127.0.0.1:8080
```

Test backend:
üëâ [http://127.0.0.1:8080/](http://127.0.0.1:8080/)

---

Running the Frontend

Just **double-click `index.html`** and open it in the browser.

Features:

* Auto-detect backend on port **8000** or **8080**
* Upload image
* See preview
* See caption + BLEU score
* Clean UI with spinner

---

Example Output

```
Caption: "A dog running across a grassy field"
BLEU Score: 0.91
```

Future Scope

* Multi-language captioning
* Faster BLIP model (BLIP-2/Flan-T5 integration)
* Cloud deployment (Render, HuggingFace Spaces)
* Mobile app integration
* Voice-based caption reading




